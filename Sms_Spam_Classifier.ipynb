{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityapatil4141/Sms-Spam-Classifier/blob/main/Sms_Spam_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fg1QbiJ6SFYI"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd # Data manipulation.\n",
        "import numpy as np # Mathematical operations.\n",
        "import matplotlib.pyplot as plt # Visualization\n",
        "import seaborn as sns # Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking scikit-learn(sklearn)'s version.\n",
        "# We need to check for 'inconsistentversion' warning.\n",
        "# In later steps when building website sklearn versions on colab and streamlit should be same.\n",
        "!pip show scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaXW1XQSuZEM",
        "outputId": "9174028c-ff5d-49e0-af90-df3c48d805c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: scikit-learn\n",
            "Version: 1.2.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: \n",
            "Author-email: \n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: joblib, numpy, scipy, threadpoolctl\n",
            "Required-by: bigframes, fastai, imbalanced-learn, librosa, mlxtend, qudida, sklearn-pandas, yellowbrick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XG6_tfIUIWs"
      },
      "outputs": [],
      "source": [
        "# Mounting google drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8i4qUzTT4w5"
      },
      "outputs": [],
      "source": [
        "# Reading DataFrame with encoding as \"ISO-8859-1\".\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/work files /sms spam classifier/spam.csv\",encoding = \"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydCK-6XWUWjy"
      },
      "outputs": [],
      "source": [
        "# Printing first 3 rows of dataset.\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGGWN8FDVCQu"
      },
      "outputs": [],
      "source": [
        "# Checking for shape of dataset.\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcUui2BNVD1D"
      },
      "outputs": [],
      "source": [
        "# Counting corresponding values of categories.\n",
        "df['v1'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQZOIxwxXN7U"
      },
      "source": [
        "#Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgLVYLBQXJcc"
      },
      "outputs": [],
      "source": [
        "# Getting dataframe information.\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ltrf7baXURE"
      },
      "outputs": [],
      "source": [
        "# Dropping unnecessary columns form the dataframe.\n",
        "df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE5HvT7iXhyx"
      },
      "outputs": [],
      "source": [
        "# Renaming remaining columns for easy understanding.\n",
        "df.rename(columns={'v1':'target','v2':'text'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QcRstm1Xw5P"
      },
      "outputs": [],
      "source": [
        "# Printing first 3 rows of dataframe.\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGVDI1YnYCxn"
      },
      "outputs": [],
      "source": [
        "# Applying LabelEncoder.\n",
        "# Label Encoder converts non-numerical values to numeriacl values.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "df['target'] =le.fit_transform(df['target'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayeV-EXVYT9t"
      },
      "outputs": [],
      "source": [
        "# Printing first 3 rows of dataset.\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4zyCzpJYZE6"
      },
      "outputs": [],
      "source": [
        "# Finding null values from the dataset.\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BRsnxhwYc2U"
      },
      "outputs": [],
      "source": [
        "# Checking for duplicates from the dataset.\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn8p22CdYgnc"
      },
      "outputs": [],
      "source": [
        "# Dropping duplicate values for the dataset,\n",
        "# (keep = ‘first’) : Drop duplicates except for the first occurrence.\n",
        "df = df.drop_duplicates(keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPHYUdE0Ym4L"
      },
      "outputs": [],
      "source": [
        "# Checking for duplicates from the dataset.\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSZU-WzcY25z"
      },
      "outputs": [],
      "source": [
        "# Checking dataframe shape.\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eycfqKsBY6Sx"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em8056oqY4rz"
      },
      "outputs": [],
      "source": [
        "# Plotting a pie chart to check which target category has highest percentage of value counts.\n",
        "plt.pie(df['target'].value_counts(),labels=['hams','spams'],autopct='%0.2f')\n",
        "plt.show()\n",
        "\n",
        "# data is imblanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1DrwbvvZhAq"
      },
      "outputs": [],
      "source": [
        "# Importing nltk.\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSzW-E6aZw4a"
      },
      "outputs": [],
      "source": [
        "# Punkt - divides a text into a list of sentences.\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB1fJIO_Z0zq"
      },
      "outputs": [],
      "source": [
        "# Finding number of characters in text column.\n",
        "df['num_characters'] = df['text'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIyxC6fKaPgh"
      },
      "outputs": [],
      "source": [
        "#num of word\n",
        "\n",
        "df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kyBv_iTajh1"
      },
      "outputs": [],
      "source": [
        "# number of sentences:\n",
        "\n",
        "df['num_sentences'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25QPOTPPbKEz"
      },
      "outputs": [],
      "source": [
        "# Printing first 3 rows of dataset.\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOI0njYEbL1h"
      },
      "outputs": [],
      "source": [
        "# Describing specific columns.\n",
        "df[['num_characters','num_words','num_sentences']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNy08ZIQbXP9"
      },
      "outputs": [],
      "source": [
        "# Describing specific columns with target==0(non-spam messages/texts).\n",
        "df[df['target']==0][['num_characters','num_words','num_sentences']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okNNV2lab-CE"
      },
      "outputs": [],
      "source": [
        "# Describing specific columns with target==0(spam messages/texts).\n",
        "df[df['target']==1][['num_characters','num_words','num_sentences']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReMH8KAhcFuZ"
      },
      "outputs": [],
      "source": [
        "# we can clearly see that spam messages average character length is bigger than ham."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCPOELbRcSx8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "sns.histplot(df[df['target']==0]['num_characters'])\n",
        "sns.histplot(df[df['target']==1]['num_characters'],color='red')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZg6oRsKdMAG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "sns.histplot(df[df['target']==0]['num_words'])\n",
        "sns.histplot(df[df['target']==1]['num_words'],color='red')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPRkrlAJdtZX"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(df.corr(),annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m34nj2cQe7Sp"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpPKI8ETqWf3"
      },
      "outputs": [],
      "source": [
        "# stopwords - words which adds no meaning to the sentence (eg - is, are, to, as, etc).\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dql_i8nCqNhr"
      },
      "outputs": [],
      "source": [
        "# Importing stop words.\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHL81s2prT2M"
      },
      "outputs": [],
      "source": [
        "# Importing PoterStemmer.\n",
        "# remove the suffixes from an English word and obtain its stem\n",
        "#Some more example of stemming for root word \"like\" include:\n",
        "# \"likes\"\n",
        "# \"liked\"\n",
        "# \"likely\"\n",
        "# \"liking\"\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oer1ozjHesDY"
      },
      "outputs": [],
      "source": [
        "# Creating a dinction function which will do following things:\n",
        "# 1. Convert text into lower format.\n",
        "# 2. Only taking alphabet-numerical (alnum) and creating new list from it.\n",
        "# 3. Removing any stopwords or puncations from the values from the list.\n",
        "# 4. Applying stemming.\n",
        "# 5. Join () - takes all the elements of an iterable and joins them into a single string.\n",
        "def text_transformer(text):\n",
        "  text = text.lower()\n",
        "  text = nltk.word_tokenize(text)\n",
        "\n",
        "  y = []\n",
        "  for i in text:\n",
        "    if i.isalnum():# alnum = alpha-numeric.\n",
        "      y.append(i)\n",
        "\n",
        "\n",
        "  text = y[:] #asssigning y to 'text' .... \"[:]\"\" we had to do it because we cannot copy list directly we have to clone it.\n",
        "  y.clear() #clearing y after assigning to text\n",
        "\n",
        "  for i in text:\n",
        "    if i not in stopwords.words('english') and i not in string.punctuation: # will check word to words(i) and see if stopword == word(i)\n",
        "      y.append(i)\n",
        "\n",
        "\n",
        "  text = y[:]\n",
        "  y.clear()\n",
        "\n",
        "  for i in text :\n",
        "    y.append(ps.stem(i))\n",
        "\n",
        "\n",
        "  return \" \".join(y)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdNudryFsQu0"
      },
      "outputs": [],
      "source": [
        "# Applying function on the dataframe.\n",
        "df['transformed_text'] = df['text'].apply(text_transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kAflin-sbqN"
      },
      "outputs": [],
      "source": [
        "# Printing first 3 rows of the dataframe.\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFvxkVj7s4ix"
      },
      "outputs": [],
      "source": [
        "# WorldCloud - a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj_yZfB2tYKH"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "wc = WordCloud(width=500, height=500,min_font_size=10,background_color='white')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5GVGQyEtz65"
      },
      "outputs": [],
      "source": [
        "span_wc =wc.generate(df[df['target']==1]['transformed_text'].str.cat(sep=\"  \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGAfjmdRumyf"
      },
      "outputs": [],
      "source": [
        "plt.imshow(span_wc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cTo748bu5XW"
      },
      "outputs": [],
      "source": [
        "ham_wc = wc.generate(df[df['target']==0]['transformed_text'].str.cat(sep=\" \")) # getting str and concatenate on space to 'transformed_text'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXzQrsvuu16I"
      },
      "outputs": [],
      "source": [
        "plt.imshow(ham_wc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeTpayyzvrak"
      },
      "outputs": [],
      "source": [
        "# getting most used word in spam:\n",
        "spam_corpus=[]\n",
        "for msg in df[df['target']==1]['transformed_text'].tolist(): #will get list of strings\n",
        "  for i in msg.split(): #iterating through every list(msg) and every word(i)\n",
        "    spam_corpus.append(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMaInNjhxQ9-"
      },
      "outputs": [],
      "source": [
        "len(spam_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1sj_hlqxPOG"
      },
      "outputs": [],
      "source": [
        "from collections import Counter #it will create a dictionary with count of occurance of each word\n",
        "# plotting barplot of 30 most common values.\n",
        "sns.barplot(x= pd.DataFrame(Counter(spam_corpus).most_common(30))[0],y =pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9x1BtNXzM6u"
      },
      "outputs": [],
      "source": [
        "ham_corpus = []\n",
        "for msg in df[df['target']==0]['transformed_text'].tolist():\n",
        "  for i in msg.split():\n",
        "    ham_corpus.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmfC-bFHzzZq"
      },
      "outputs": [],
      "source": [
        "ham_counter = Counter(ham_corpus).most_common(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i21hTOazpwX"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x=pd.DataFrame(ham_counter)[0],y=pd.DataFrame(ham_counter)[1])\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw2j44Q_0bSo"
      },
      "source": [
        "#Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxruWQPR0cyW"
      },
      "outputs": [],
      "source": [
        "# We know that naiveBayes algorithm works best on textual data:\n",
        "# NaiveBayes need numerical data,\n",
        "# We have to convert text to numerical data/vectors,\n",
        "# Bagsofword(frequent word) ,tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMRO6YKa1WI5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=3000)\n",
        "cv = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxEHQMU_1mtG"
      },
      "outputs": [],
      "source": [
        "x = tfidf.fit_transform(df['transformed_text']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKiUwUSo2BOT"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6oRY9qY2BsR"
      },
      "outputs": [],
      "source": [
        "y = df['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpSC6ZSd1rZe"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXmfZ6bc9BO9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-jMLDq-9LSu"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf2R-0ao9VQa"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix,precision_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgee73VK9mvA"
      },
      "outputs": [],
      "source": [
        "gnb =GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "bnb = BernoulliNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxeSN2a295j9"
      },
      "outputs": [],
      "source": [
        "gnb.fit(x_train,y_train)\n",
        "y_pred1 = gnb.predict(x_test)\n",
        "print(accuracy_score(y_test,y_pred1))\n",
        "print(confusion_matrix(y_test,y_pred1))\n",
        "print(precision_score(y_test,y_pred1 ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvHMz2vU-4l0"
      },
      "outputs": [],
      "source": [
        "mnb.fit(x_train,y_train)\n",
        "y_pred2 = mnb.predict(x_test)\n",
        "print(accuracy_score(y_test,y_pred2))\n",
        "print(confusion_matrix(y_test,y_pred2))\n",
        "print(precision_score(y_test,y_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqI5NDSr_FxX"
      },
      "outputs": [],
      "source": [
        "#till noe bernoulli is performing well\n",
        "\n",
        "bnb.fit(x_train,y_train)\n",
        "y_pred3 = bnb.predict(x_test)\n",
        "print(accuracy_score(y_test,y_pred3))\n",
        "print(confusion_matrix(y_test,y_pred3))\n",
        "print(precision_score(y_test,y_pred3 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGqELGF4X1RZ"
      },
      "source": [
        "WITH TFIDF VECTORIZER:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCK5bzGTSXlR"
      },
      "outputs": [],
      "source": [
        "# WITH TFIDF VECTORIZER:\n",
        "\n",
        "x = tfidf.fit_transform(df['transformed_text']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkWjD1asTGMj"
      },
      "outputs": [],
      "source": [
        "y = df['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMu5FLWl-brE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI8mLEvZTR1p"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF7y5aX1-clS"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ajtes9GTjAY"
      },
      "outputs": [],
      "source": [
        "gnb.fit(x_train,y_train)\n",
        "y_pred4 = gnb.predict(x_test)\n",
        "print(accuracy_score(y_test,y_pred4))\n",
        "print(confusion_matrix(y_test,y_pred4))\n",
        "print(precision_score(y_test,y_pred4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aoe4Vn0YUB50"
      },
      "outputs": [],
      "source": [
        "mnb.fit(x_train,y_train)\n",
        "y_pred5 = mnb.predict(x_test)\n",
        "print(accuracy_score(y_test,y_pred5))\n",
        "print(confusion_matrix(y_test,y_pred5))\n",
        "print(precision_score(y_test,y_pred5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-NqZQZ7US-T"
      },
      "outputs": [],
      "source": [
        "bnb.fit(x_train,y_train)\n",
        "y_pred6 = bnb.predict(x_test)\n",
        "print(accuracy_score(y_test,y_pred6))\n",
        "print(confusion_matrix(y_test,y_pred6))\n",
        "print(precision_score(y_test,y_pred6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oh7GM8MYNSy"
      },
      "outputs": [],
      "source": [
        "# Here we choose TfidfVectorizer --> MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHDNIqBHZi3C"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM6PF1xFbHGy"
      },
      "outputs": [],
      "source": [
        "svc = SVC(kernel = 'sigmoid',gamma=1.0) #Support Vector Classification\n",
        "mlb = MultinomialNB()\n",
        "knc = KNeighborsClassifier()\n",
        "dtc = DecisionTreeClassifier(max_depth=5)\n",
        "lrc = LogisticRegression(solver = 'liblinear',penalty='l1')\n",
        "rfc = RandomForestClassifier(n_estimators = 50, random_state = 2)\n",
        "abc = AdaBoostClassifier(n_estimators = 50, random_state = 2)\n",
        "bc = BaggingClassifier(n_estimators = 50, random_state = 2)\n",
        "etc = ExtraTreesClassifier(n_estimators = 50 , random_state = 2)\n",
        "gbc = GradientBoostingClassifier(n_estimators = 50, random_state = 2)\n",
        "xgc = XGBClassifier(n_estimators = 50 , random_State = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnsFKXNU6oHd"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus.reader import et\n",
        "clfs = {\n",
        "    'svc' : svc,\n",
        "    'Nb' : mlb,\n",
        "    'knc' : knc,\n",
        "    'dtc' : dtc,\n",
        "    'lrc' : lrc,\n",
        "    'rfc' : rfc,\n",
        "    'abc' : abc,\n",
        "    'bc' : bc,\n",
        "    'etc' : etc,\n",
        "    'gbc' : gbc,\n",
        "    'xgc' : xgc\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFzxKHgwvrST"
      },
      "outputs": [],
      "source": [
        "def train_classifier(clf, x_train,y_train,x_test,y_test):\n",
        "  clf.fit(x_train,y_train)\n",
        "  clf.predict(x_test)\n",
        "  y_pred  = clf.predict(x_test)\n",
        "  accuracy = accuracy_score(y_test,y_pred)\n",
        "  precision = precision_score(y_test,y_pred)\n",
        "\n",
        "  return accuracy , precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic5TMccZ1KOW"
      },
      "outputs": [],
      "source": [
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "\n",
        "for name, clf in clfs.items():\n",
        "  current_accuracy,current_precision = train_classifier(clf,x_train,y_train,x_test,y_test)\n",
        "  print('for', name)\n",
        "  print('accuracy = ',current_accuracy)\n",
        "  print('precision = ',current_precision)\n",
        "\n",
        "  accuracy_scores.append(current_accuracy)\n",
        "  precision_scores.append(current_precision)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzp6chLe2-4r"
      },
      "outputs": [],
      "source": [
        "performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzHieL9hUJxA"
      },
      "outputs": [],
      "source": [
        "performance_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k02LZ-ZnYtLW"
      },
      "outputs": [],
      "source": [
        "performance_df1 = pd.melt(performance_df,id_vars='Algorithm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xazoiYijd4uZ"
      },
      "outputs": [],
      "source": [
        "performance_df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq0xu1QIgWjG"
      },
      "outputs": [],
      "source": [
        "sns.catplot(x = 'Algorithm', y = 'value', hue='variable',data=performance_df1, kind='bar',height=5)\n",
        "plt.ylim(0.5,1.0)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(tfidf,open('vectorizer2.pkl','wb'))\n",
        "pickle.dump(mnb,open('model2.pkl','wb'))"
      ],
      "metadata": {
        "id": "mwl3hdri6lW5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rQZOIxwxXN7U",
        "eycfqKsBY6Sx",
        "m34nj2cQe7Sp"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyOKmz+zIgf/hLH6ZLXqauYz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}